-- Computable network gradient via manual backpropagation
-- From: VerifiedNN/Network/ManualGradient.lean

def networkGradientManual
  (params : Vector nParams)
  (input : Vector 784)
  (target : Nat)
  : Vector nParams :=

  -- ===== FORWARD PASS (save activations) =====

  -- Unflatten parameters into network structure
  let net := unflattenParams params

  -- Layer 1: Dense layer forward
  let z1 := net.layer1.forwardLinear input
    -- [128] pre-activation (SAVE for ReLU backward)

  -- ReLU activation
  let h1 := reluVec z1
    -- [128] post-activation (SAVE for layer 2 backward)

  -- Layer 2: Dense layer forward
  let z2 := net.layer2.forwardLinear h1
    -- [10] logits (SAVE for loss backward)

  -- Note: We don't compute softmax explicitly here since
  -- lossGradient combines softmax+cross-entropy gradient
  -- (more numerically stable)

  -- ===== BACKWARD PASS (chain rule) =====

  -- Start at the loss: gradient of cross-entropy w.r.t. logits
  -- This is the beautiful formula: softmax(z) - one_hot(target)
  let dL_dz2 := lossGradient z2 target  -- [10]

  -- Backprop through layer 2 (dense)
  let (dW2, db2, dL_dh1) :=
    denseLayerBackward dL_dz2 h1 net.layer2.weights
  -- dW2: [10, 128] weight gradient
  -- db2: [10] bias gradient
  -- dL_dh1: [128] gradient flowing back to hidden layer

  -- Backprop through ReLU activation
  -- Gradient flows through where z1 > 0, zeroed elsewhere
  let dL_dz1 := reluBackward dL_dh1 z1  -- [128]

  -- Backprop through layer 1 (dense)
  let (dW1, db1, _dL_dinput) :=
    denseLayerBackward dL_dz1 input net.layer1.weights
  -- dW1: [128, 784] weight gradient
  -- db1: [128] bias gradient
  -- We don't need dL_dinput since input is not trainable

  -- ===== PACK GRADIENTS =====

  -- Flatten all gradients into single parameter vector
  GradientFlattening.flattenGradients dW1 db1 dW2 db2
    -- [101,770] total parameters
